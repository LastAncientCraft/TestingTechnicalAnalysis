{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fb96f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "asset_list = [\n",
    "    \"SPY\", \"QQQ\", \"DIA\",\n",
    "    \"EWJ\", \"EEM\", \"VGK\",\n",
    "    \"TLT\", \"IEF\", \"BND\",\n",
    "    \"GLD\", \"SLV\", \"USO\",\n",
    "    \"EURUSD=X\", \"JPY=X\", \"AUDUSD=X\",\n",
    "    \"BTC-USD\", \"ETH-USD\"\n",
    "]\n",
    "\n",
    "file_path = \"backtest_by_strategy_{strategy_name}.xlsx\"\n",
    "\n",
    "def parse_perf(x):\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "\n",
    "    if isinstance(x, dict):\n",
    "        return x\n",
    "\n",
    "    if not isinstance(x, str):\n",
    "        raise TypeError(f\"Unexpected type: {type(x)}\")\n",
    "\n",
    "    s = x\n",
    "\n",
    "    # 1. Remove numpy wrappers\n",
    "    s = re.sub(r'np\\.float64\\(([^)]+)\\)', r'\\1', s)\n",
    "\n",
    "    # 2. Replace invalid literals BEFORE parsing\n",
    "    s = re.sub(r'\\bNaN\\b|\\bnan\\b', 'None', s)\n",
    "    s = re.sub(r'\\binf\\b|\\b-inf\\b', 'None', s)\n",
    "\n",
    "    # 3. Remove unary minus applied to None (all forms)\n",
    "    s = re.sub(r'-\\s*\\(?\\s*None\\s*\\)?', 'None', s)\n",
    "\n",
    "    try:\n",
    "        d = ast.literal_eval(s)\n",
    "    except Exception:\n",
    "        print(\"FAILED STRING ↓↓↓\")\n",
    "        print(s)\n",
    "        raise\n",
    "\n",
    "    # 4. Final numeric sanitation (post-parse)\n",
    "    for k, v in d.items():\n",
    "        if v is None:\n",
    "            continue\n",
    "        if isinstance(v, float) and (np.isnan(v) or np.isinf(v)):\n",
    "            d[k] = None\n",
    "\n",
    "    return d\n",
    "\n",
    "def extract_ssr_ftest_values(granger_str):\n",
    "    if not isinstance(granger_str, str):\n",
    "        return np.empty((0, 4))  # no tests found\n",
    "\n",
    "    matches = re.findall(\n",
    "        r\"'ssr_ftest'\\s*:\\s*\\(\\s*\"\n",
    "        r\"np\\.float64\\(([\\d.eE+-]+)\\)\\s*,\\s*\"\n",
    "        r\"np\\.float64\\(([\\d.eE+-]+)\\)\\s*,\\s*\"\n",
    "        r\"np\\.float64\\(([\\d.eE+-]+)\\)\\s*,\\s*\"\n",
    "        r\"np\\.int64\\(([\\d.eE+-]+)\\)\\s*\"\n",
    "        r\"\\)\",\n",
    "        granger_str\n",
    "    )\n",
    "\n",
    "    return np.array(matches, dtype=float)\n",
    "\n",
    "def parse_beta(x):\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    \n",
    "    if isinstance(x, dict):\n",
    "        return x\n",
    "    \n",
    "    if isinstance(x, str):\n",
    "        x = x.replace(\"np.float64(\", \"\").replace(\")\", \"\")\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "def avg_pvalue_from_ssr(ssr):\n",
    "    if ssr.size == 0:\n",
    "        return np.nan\n",
    "    return ssr[:, 1].mean()   # column 1 = p-value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f50bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "for asset in asset_list:\n",
    "    sheet = f\"{asset}_{strategy_name}\"\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet)\n",
    "    df[\"perf_dict\"] = df[\"perf_metric_0\"].apply(parse_perf)\n",
    "    metrics_df = pd.json_normalize(df[\"perf_dict\"])\n",
    "    df = pd.concat([df, metrics_df], axis=1)\n",
    "    df[\"factor_regression_beta_dict\"] = df[\"factor_regression_beta\"].apply(parse_beta)\n",
    "    betas = pd.json_normalize(df[\"factor_regression_beta_dict\"])\n",
    "    df = pd.concat([df, betas], axis=1)\n",
    "    df[\"binom_pvalue\"] = (\n",
    "        df[\"accuracy_binom_p\"]\n",
    "        .str.extract(r\"pvalue=([0-9.eE+-]+)\")\n",
    "        .astype(float)\n",
    "    )\n",
    "    df[\"avg_granger1_pvalue\"] = (\n",
    "        df[\"signal_causes_returns\"]\n",
    "        .apply(lambda x: avg_pvalue_from_ssr(extract_ssr_ftest_values(x)))\n",
    "    )\n",
    "\n",
    "    df[\"avg_granger2_pvalue\"] = (\n",
    "        df[\"returns_cause_signal\"]\n",
    "        .apply(lambda x: avg_pvalue_from_ssr(extract_ssr_ftest_values(x)))\n",
    "    )\n",
    "    data[asset] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af64c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics = []\n",
    "valid_values = []\n",
    "\n",
    "for asset in asset_list:\n",
    "    df = data[asset]\n",
    "    df[[\"CAGR\", \"Max Drawdown\", \"jensens_alpha\"]] = df[[\"CAGR\", \"Max Drawdown\", \"jensens_alpha\"]].fillna(0)\n",
    "    nan_ratio = df.isna().sum().sum() / (df.shape[0] * df.shape[1])\n",
    "    metrics = {\n",
    "        \"asset\": asset,\n",
    "        \"mean_cagr\": df[\"CAGR\"].mean(),\n",
    "        \"median_max_drawdown\": df[\"Max Drawdown\"].median(),\n",
    "        \"mean_sharpe\": df[\"Sharpe Ratio\"].mean(),\n",
    "        \"mean_sortino\": df[\"Sortino Ratio\"].mean(),\n",
    "        \"mean_profit_factor\": df[\"Profit Factor\"].mean(),\n",
    "        \"mean_30d_rolling_sharpe\": df[\"30 days Rolling Sharpe\"].mean(),\n",
    "        \"mean_jensens_alpha\": df[\"jensens_alpha\"].mean(),\n",
    "        \"nan_ratio\": nan_ratio\n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    performance_metrics.append(metrics)\n",
    "\n",
    "performance_df = pd.DataFrame(performance_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f7c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0773ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.05\n",
    "significance_metrics = []\n",
    "significance_counts = []\n",
    "\n",
    "for asset in asset_list:\n",
    "    df = data[asset]\n",
    "\n",
    "    count_ttest_parameters_below_threshold = (df[\"paired_t_p\"] < threshold).sum()\n",
    "    count_valid_ttest_values = df[\"paired_t_p\"].notna().sum()\n",
    "    percentage_ttest_parameters_below_threshold = (count_ttest_parameters_below_threshold / count_valid_ttest_values) * 100\n",
    "\n",
    "    count_wilcoxon_parameters_below_threshold = (df[\"wilcoxon_p\"] < threshold).sum()\n",
    "    count_valid_wilcoxon_values = df[\"wilcoxon_p\"].notna().sum()\n",
    "    percentage_wilcoxon_parameters_below_threshold = (count_wilcoxon_parameters_below_threshold / count_valid_wilcoxon_values) * 100\n",
    "\n",
    "    count_newey_west_parameters_below_threshold = (df[\"nw_alpha_pvalue\"] < threshold).sum()\n",
    "    count_valid_newey_west_values = df[\"nw_alpha_pvalue\"].notna().sum()\n",
    "    percentage_newey_west_parameters_below_threshold = (count_newey_west_parameters_below_threshold / count_valid_newey_west_values) * 100\n",
    "\n",
    "    count_ledoit_wolf_parameters_below_threshold = (df[\"lw_sharpe_pvalue\"] < threshold).sum()\n",
    "    count_valid_ledoit_wolf_values = df[\"lw_sharpe_pvalue\"].notna().sum()\n",
    "    percentage_ledoit_wolf_parameters_below_threshold = (count_ledoit_wolf_parameters_below_threshold / count_valid_ledoit_wolf_values) * 100\n",
    "\n",
    "    count_bootstrap_significance_parameters_below_threshold = (df[\"bootstrap_pvalue\"] < threshold).sum()\n",
    "    count_valid_bootstrap_values = df[\"bootstrap_pvalue\"].notna().sum()\n",
    "    percentage_bootstrap_parameters_below_threshold = (count_bootstrap_significance_parameters_below_threshold / count_valid_bootstrap_values) * 100\n",
    "\n",
    "    count_directional_accuracy_significant_positive_parameters = ((df[\"accuracy\"] > 0.5) & (df[\"binom_pvalue\"] < threshold)).sum()\n",
    "    count_directional_accuracy_significant_negative_parameters = ((df[\"accuracy\"] < 0.5) & (df[\"binom_pvalue\"] < threshold)).sum()\n",
    "    count_directional_accuracy_insignificant_parameters = (df[\"binom_pvalue\"] >= threshold).sum()\n",
    "    count_valid_accuracy_values = df[\"accuracy\"].notna().sum()\n",
    "    count_valid_binom_pvalue_values = df[\"binom_pvalue\"].notna().sum()\n",
    "    count_valid_dirrectional_accuracy_values = count_valid_accuracy_values + count_valid_binom_pvalue_values\n",
    "    percentage_directional_accuracy_significant_positive_parameters = (count_directional_accuracy_significant_positive_parameters / count_valid_dirrectional_accuracy_values) * 100\n",
    "    percentage_directional_accuracy_significant_negative_parameters = (count_directional_accuracy_significant_negative_parameters / count_valid_dirrectional_accuracy_values) * 100\n",
    "    percentage_directional_accuracy_insignificant_parameters = (count_directional_accuracy_insignificant_parameters / count_valid_dirrectional_accuracy_values) * 100\n",
    "\n",
    "\n",
    "    count_granger1_parameters_below_threshold = (df[\"avg_granger1_pvalue\"] < threshold).sum()\n",
    "    count_valid_granger1_values = df[\"avg_granger1_pvalue\"].notna().sum()\n",
    "    percentage_granger1_parameters_below_threshold = (count_granger1_parameters_below_threshold / count_valid_granger1_values) * 100\n",
    "\n",
    "\n",
    "    count_granger2_parameters_below_threshold = (df[\"avg_granger2_pvalue\"] < threshold).sum()\n",
    "    count_valid_granger2_values = df[\"avg_granger2_pvalue\"].notna().sum()\n",
    "    percentage_granger2_parameters_below_threshold = (count_granger2_parameters_below_threshold / count_valid_granger2_values) * 100\n",
    "    \n",
    "\n",
    "    count_random_permutation_parameters_below_threshold = (df[\"permutation_pvalue\"] < threshold).sum()\n",
    "    count_valid_random_permutation_values = df[\"permutation_pvalue\"].notna().sum()\n",
    "    percentage_random_permutation_parameters_below_threshold = (count_random_permutation_parameters_below_threshold / count_valid_random_permutation_values) * 100\n",
    "\n",
    "\n",
    "    count_innovation_test_parameters_below_threshold = (df[\"innovation_test_pvalue\"] < threshold).sum()\n",
    "    count_valid_innovation_test_values = df[\"innovation_test_pvalue\"].notna().sum()\n",
    "    percentage_innovation_test_parameters_below_threshold = (count_innovation_test_parameters_below_threshold / count_valid_innovation_test_values) * 100\n",
    "\n",
    "\n",
    "    count_adf_test_parameters_below_threshold = (df[\"adf_p\"] < threshold).sum()\n",
    "    count_valid_adf_values = df[\"adf_p\"].notna().sum()\n",
    "    percentage_adf_test_parameters_below_threshold = (count_adf_test_parameters_below_threshold / count_valid_adf_values) * 100\n",
    "\n",
    "    count_kpss_test_parameters_below_threshold = (df[\"kpss_p\"] < threshold).sum()\n",
    "    count_valid_kpss_values = df[\"kpss_p\"].notna().sum()\n",
    "    percentage_kpss_test_parameters_below_threshold = (count_kpss_test_parameters_below_threshold / count_valid_kpss_values) * 100\n",
    "\n",
    "    mean_factor_reg_alpha_across_parameters = df[\"factor_regression_alpha\"].mean()\n",
    "    median_factor_reg_alpha_across_parameters = df[\"factor_regression_alpha\"].median()\n",
    "    count_factor_reg_alpha_parameters_below_threshold = (df[\"factor_regression_alpha_pvalue\"] < threshold).sum()\n",
    "    count_valid_factor_reg_alpha_values = df[\"factor_regression_alpha\"].notna().sum()\n",
    "    count_valid_factor_reg_alpha_pvalues_values = df[\"factor_regression_alpha_pvalue\"].notna().sum()\n",
    "    percentage_factor_reg_alpha_parameters_below_threshold = (count_factor_reg_alpha_parameters_below_threshold / count_valid_factor_reg_alpha_pvalues_values) * 100\n",
    "\n",
    "\n",
    "    metrics ={\n",
    "        \"asset\": asset,\n",
    "        \"percentage_ttest_below_0.05\": percentage_ttest_parameters_below_threshold,\n",
    "        \"percentage_wilcoxon_below_0.05\": percentage_wilcoxon_parameters_below_threshold,\n",
    "        \"percentage_newey_west_below_0.05\": percentage_newey_west_parameters_below_threshold,\n",
    "        \"percentage_ledoit_wolf_below_0.05\": percentage_ledoit_wolf_parameters_below_threshold,\n",
    "        \"percentage_bootstrap_below_0.05\": percentage_bootstrap_parameters_below_threshold,\n",
    "        \"percentage_directional_accuracy_significant_positive\": percentage_directional_accuracy_significant_positive_parameters,\n",
    "        \"percentage_directional_accuracy_significant_negative\": percentage_directional_accuracy_significant_negative_parameters,\n",
    "        \"percentage_directional_accuracy_insignificant\": percentage_directional_accuracy_insignificant_parameters,\n",
    "        \"percentage_granger1_below_0.05\": percentage_granger1_parameters_below_threshold,\n",
    "        \"percentage_granger2_below_0.05\": percentage_granger2_parameters_below_threshold,\n",
    "        \"percentage_random_permutation_below_0.05\": percentage_random_permutation_parameters_below_threshold,\n",
    "        \"percentage_innovation_test_below_0.05\": percentage_innovation_test_parameters_below_threshold,\n",
    "        \"percentage_adf_test_below_0.05\": percentage_adf_test_parameters_below_threshold,\n",
    "        \"percentage_kpss_test_below_0.05\": percentage_kpss_test_parameters_below_threshold,\n",
    "        \"mean_factor_reg_alpha_across_parameters\": mean_factor_reg_alpha_across_parameters,\n",
    "        \"median_factor_reg_alpha_across_parameters\": median_factor_reg_alpha_across_parameters,\n",
    "        \"percentage_factor_reg_alpha_across_parameters\": percentage_factor_reg_alpha_parameters_below_threshold\n",
    "    }\n",
    "\n",
    "    counts ={\n",
    "        \"asset\": asset,\n",
    "        \"count_ttest_below_0.05\": count_ttest_parameters_below_threshold,\n",
    "        \"count_ttest_valid_values\": count_valid_ttest_values,\n",
    "        \"count_wilcoxon_below_0.05\": count_wilcoxon_parameters_below_threshold,\n",
    "        \"count_wilcoxon_valid_values\": count_valid_wilcoxon_values,\n",
    "        \"count_newey_west_below_0.05\": count_newey_west_parameters_below_threshold,\n",
    "        \"count_newey_west_valid_values\": count_valid_newey_west_values,\n",
    "        \"count_ledoit_wolf_below_0.05\": count_ledoit_wolf_parameters_below_threshold,\n",
    "        \"count_ledoit_wolf_valid_values\": count_valid_ledoit_wolf_values,\n",
    "        \"count_bootstrap_below_0.05\": count_bootstrap_significance_parameters_below_threshold,\n",
    "        \"count_bootstrap_valid_values\": count_valid_bootstrap_values,\n",
    "        \"count_directional_accuracy_significant_positive\": count_directional_accuracy_significant_positive_parameters,\n",
    "        \"count_directional_accuracy_significant_negative\": count_directional_accuracy_significant_negative_parameters,\n",
    "        \"count_directional_accuracy_insignificant\": count_directional_accuracy_insignificant_parameters,\n",
    "        \"count_granger1_below_0.05\": count_granger1_parameters_below_threshold,\n",
    "        \"count_granger1_valid_values\": count_valid_granger1_values,\n",
    "        \"count_granger2_below_0.05\": count_granger2_parameters_below_threshold,\n",
    "        \"count_granger2_valid_values\": count_valid_granger2_values,\n",
    "        \"count_random_permutation_below_0.05\": count_random_permutation_parameters_below_threshold,\n",
    "        \"count_random_permutation_valid_values\": count_valid_random_permutation_values,\n",
    "        \"count_innovation_test_below_0.05\": count_innovation_test_parameters_below_threshold,\n",
    "        \"count_innovation_test_valid_values\": count_valid_innovation_test_values,\n",
    "        \"count_adf_test_below_0.05\": count_adf_test_parameters_below_threshold,\n",
    "        \"count_adf_test_valid_values\": count_valid_adf_values,\n",
    "        \"count_kpss_test_below_0.05\": count_kpss_test_parameters_below_threshold,\n",
    "        \"count_kpss_test_valid_values\": count_valid_kpss_values,\n",
    "        \"count_factor_reg_alpha_below_0.05\": count_factor_reg_alpha_parameters_below_threshold,\n",
    "        \"count_valid_factor_reg_alpha_values\": count_valid_factor_reg_alpha_values,\n",
    "        \"count_valid_factor_reg_alpha_pvalues_values\": count_valid_factor_reg_alpha_pvalues_values\n",
    "    }\n",
    "\n",
    "    significance_metrics.append(metrics)\n",
    "    significance_counts.append(counts)\n",
    "\n",
    "significance_df = pd.DataFrame(significance_metrics)\n",
    "significance_counts_df = pd.DataFrame(significance_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a38752",
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5dea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357d6f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_vol_df = []\n",
    "\n",
    "for asset in asset_list:\n",
    "    df = data[asset]\n",
    "    mean_hvol_strategy_total_returns = df[\"hvol_strategy total return\"].mean()\n",
    "    median_hvol_strategy_total_returns = df[\"hvol_strategy total return\"].median()\n",
    "    mean_hvol_strategy_annualized_return = df[\"hvol_ann. strategy return\"].mean()\n",
    "    median_hvol_strategy_annualized_return = df[\"hvol_ann. strategy return\"].median()\n",
    "    mean_hvol_strategy_volatility = df[\"hvol_ann. strategy volatility\"].mean()\n",
    "    median_hvol_strategy_volatility = df[\"hvol_ann. strategy volatility\"].median()\n",
    "    mean_hvol_strategy_sharpe = df[\"hvol_strategy sharpe ratio\"].mean()\n",
    "    median_hvol_strategy_sharpe = df[\"hvol_strategy sharpe ratio\"].median()\n",
    "    mean_hvol_strategy_winrate = df[\"hvol_strategy winrate\"].mean()\n",
    "    median_hvol_strategy_winrate = df[\"hvol_strategy winrate\"].median()\n",
    "\n",
    "    metrics = {\n",
    "        \"asset\": asset,\n",
    "        \"mean_hvol_strategy_total_returns\": mean_hvol_strategy_total_returns,\n",
    "        \"median_hvol_strategy_total_returns\": median_hvol_strategy_total_returns,\n",
    "        \"mean_hvol_strategy_annualized_return\": mean_hvol_strategy_annualized_return,\n",
    "        \"median_hvol_strategy_annualized_return\": median_hvol_strategy_annualized_return,\n",
    "        \"mean_hvol_strategy_volatility\": mean_hvol_strategy_volatility,\n",
    "        \"median_hvol_strategy_volatility\": median_hvol_strategy_volatility,\n",
    "        \"mean_hvol_strategy_sharpe\": mean_hvol_strategy_sharpe,\n",
    "        \"median_hvol_strategy_sharpe\": median_hvol_strategy_sharpe,\n",
    "        \"mean_hvol_strategy_winrate\": mean_hvol_strategy_winrate,\n",
    "        \"median_hvol_strategy_winrate\": median_hvol_strategy_winrate\n",
    "    }\n",
    "    high_vol_df.append(metrics)\n",
    "\n",
    "high_vol_df = pd.DataFrame(high_vol_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98996ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_vol_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b00bf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_vol_df = []\n",
    "\n",
    "for asset in asset_list:\n",
    "    df = data[asset]\n",
    "    mean_lvol_strategy_total_returns = df[\"lvol_strategy total return\"].mean()\n",
    "    median_lvol_strategy_total_returns = df[\"lvol_strategy total return\"].median()\n",
    "    mean_lvol_strategy_annualized_return = df[\"lvol_ann. strategy return\"].mean()\n",
    "    median_lvol_strategy_annualized_return = df[\"lvol_ann. strategy return\"].median()\n",
    "    mean_lvol_strategy_volatility = df[\"lvol_ann. strategy volatility\"].mean()\n",
    "    median_lvol_strategy_volatility = df[\"lvol_ann. strategy volatility\"].median()\n",
    "    mean_lvol_strategy_sharpe = df[\"lvol_strategy sharpe ratio\"].mean()\n",
    "    median_lvol_strategy_sharpe = df[\"lvol_strategy sharpe ratio\"].median()\n",
    "    mean_lvol_strategy_winrate = df[\"lvol_strategy winrate\"].mean()\n",
    "    median_lvol_strategy_winrate = df[\"lvol_strategy winrate\"].median()\n",
    "\n",
    "    metrics = {\n",
    "        \"asset\": asset,\n",
    "        \"mean_lvol_strategy_total_returns\": mean_lvol_strategy_total_returns,\n",
    "        \"median_lvol_strategy_total_returns\": median_lvol_strategy_total_returns,\n",
    "        \"mean_lvol_strategy_annualized_return\": mean_lvol_strategy_annualized_return,\n",
    "        \"median_lvol_strategy_annualized_return\": median_lvol_strategy_annualized_return,\n",
    "        \"mean_lvol_strategy_volatility\": mean_lvol_strategy_volatility,\n",
    "        \"median_lvol_strategy_volatility\": median_lvol_strategy_volatility,\n",
    "        \"mean_lvol_strategy_sharpe\": mean_lvol_strategy_sharpe,\n",
    "        \"median_lvol_strategy_sharpe\": median_lvol_strategy_sharpe,\n",
    "        \"mean_lvol_strategy_winrate\": mean_lvol_strategy_winrate,\n",
    "        \"median_lvol_strategy_winrate\": median_lvol_strategy_winrate\n",
    "    }\n",
    "    low_vol_df.append(metrics)\n",
    "\n",
    "low_vol_df = pd.DataFrame(low_vol_df)\n",
    "low_vol_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0db242",
   "metadata": {},
   "outputs": [],
   "source": [
    "spy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c325f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_vol_buy_n_hold_df = []\n",
    "\n",
    "for asset in asset_list:\n",
    "    df = data[asset]\n",
    "    mean_hvol_buy_n_hold_total_returns = df[\"hvol_ann. buy n hold return\"].mean()\n",
    "    median_hvol_buy_n_hold_total_returns = df[\"hvol_ann. buy n hold return\"].median()\n",
    "    mean_hvol_buy_n_hold_annualized_return = df[\"hvol_ann. buy n hold return\"].mean()\n",
    "    median_hvol_buy_n_hold_annualized_return = df[\"hvol_ann. buy n hold return\"].median()\n",
    "    mean_hvol_buy_n_hold_volatility = df[\"hvol_ann. buy n hold volatility\"].mean()\n",
    "    median_hvol_buy_n_hold_volatility = df[\"hvol_ann. buy n hold volatility\"].median()\n",
    "    mean_hvol_buy_n_hold_sharpe = df[\"hvol_buy n hold sharpe ratio\"].mean()\n",
    "    median_hvol_buy_n_hold_sharpe = df[\"hvol_buy n hold sharpe ratio\"].median()\n",
    "\n",
    "    metrics = {\n",
    "        \"asset\": asset,\n",
    "        \"mean_hvol_buy_n_hold_total_returns\": mean_hvol_buy_n_hold_total_returns,\n",
    "        \"median_hvol_buy_n_hold_total_returns\": median_hvol_buy_n_hold_total_returns,\n",
    "        \"mean_hvol_buy_n_hold_annualized_return\": mean_hvol_buy_n_hold_annualized_return,\n",
    "        \"median_hvol_buy_n_hold_annualized_return\": median_hvol_buy_n_hold_annualized_return,\n",
    "        \"mean_hvol_buy_n_hold_volatility\": mean_hvol_buy_n_hold_volatility,\n",
    "        \"median_hvol_buy_n_hold_volatility\": median_hvol_buy_n_hold_volatility,\n",
    "        \"mean_hvol_buy_n_hold_sharpe\": mean_hvol_buy_n_hold_sharpe,\n",
    "        \"median_hvol_buy_n_hold_sharpe\": median_hvol_buy_n_hold_sharpe,\n",
    "    }\n",
    "    high_vol_buy_n_hold_df.append(metrics)\n",
    "\n",
    "high_vol_buy_n_hold_df = pd.DataFrame(high_vol_buy_n_hold_df)\n",
    "high_vol_buy_n_hold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45779b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_vol_buy_n_hold_df = []\n",
    "\n",
    "for asset in asset_list:\n",
    "    df = data[asset]\n",
    "    mean_lvol_buy_n_hold_total_returns = df[\"lvol_ann. buy n hold return\"].mean()\n",
    "    median_lvol_buy_n_hold_total_returns = df[\"lvol_ann. buy n hold return\"].median()\n",
    "    mean_lvol_buy_n_hold_annualized_return = df[\"lvol_ann. buy n hold return\"].mean()\n",
    "    median_lvol_buy_n_hold_annualized_return = df[\"lvol_ann. buy n hold return\"].median()\n",
    "    mean_lvol_buy_n_hold_volatility = df[\"lvol_ann. buy n hold volatility\"].mean()\n",
    "    median_lvol_buy_n_hold_volatility = df[\"lvol_ann. buy n hold volatility\"].median()\n",
    "    mean_lvol_buy_n_hold_sharpe = df[\"lvol_buy n hold sharpe ratio\"].mean()\n",
    "    median_lvol_buy_n_hold_sharpe = df[\"lvol_buy n hold sharpe ratio\"].median()\n",
    "\n",
    "    metrics = {\n",
    "        \"asset\": asset,\n",
    "        \"mean_lvol_buy_n_hold_total_returns\": mean_lvol_buy_n_hold_total_returns,\n",
    "        \"median_lvol_buy_n_hold_total_returns\": median_lvol_buy_n_hold_total_returns,\n",
    "        \"mean_lvol_buy_n_hold_annualized_return\": mean_lvol_buy_n_hold_annualized_return,\n",
    "        \"median_lvol_buy_n_hold_annualized_return\": median_lvol_buy_n_hold_annualized_return,\n",
    "        \"mean_lvol_buy_n_hold_volatility\": mean_lvol_buy_n_hold_volatility,\n",
    "        \"median_lvol_buy_n_hold_volatility\": median_lvol_buy_n_hold_volatility,\n",
    "        \"mean_lvol_buy_n_hold_sharpe\": mean_lvol_buy_n_hold_sharpe,\n",
    "        \"median_lvol_buy_n_hold_sharpe\": median_lvol_buy_n_hold_sharpe,\n",
    "    }\n",
    "    low_vol_buy_n_hold_df.append(metrics)\n",
    "\n",
    "low_vol_buy_n_hold_df = pd.DataFrame(low_vol_buy_n_hold_df)\n",
    "low_vol_buy_n_hold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84f9827",
   "metadata": {},
   "outputs": [],
   "source": [
    "recession_df = []\n",
    "\n",
    "for asset in asset_list:\n",
    "    df = data[asset]\n",
    "    mean_rec_strategy_total_returns = df[\"rec_strategy total return\"].mean()\n",
    "    median_rec_strategy_total_returns = df[\"rec_strategy total return\"].median()\n",
    "    mean_rec_strategy_annualized_return = df[\"rec_ann. strategy return\"].mean()\n",
    "    median_rec_strategy_annualized_return = df[\"rec_ann. strategy return\"].median()\n",
    "    mean_rec_strategy_volatility = df[\"rec_ann. strategy volatility\"].mean()\n",
    "    median_rec_strategy_volatility = df[\"rec_ann. strategy volatility\"].median()\n",
    "    mean_rec_strategy_sharpe = df[\"rec_strategy sharpe ratio\"].mean()\n",
    "    median_rec_strategy_sharpe = df[\"rec_strategy sharpe ratio\"].median()\n",
    "    mean_rec_strategy_winrate = df[\"rec_strategy winrate\"].mean()\n",
    "    median_rec_strategy_winrate = df[\"rec_strategy winrate\"].median()\n",
    "\n",
    "    metrics = {\n",
    "        \"asset\": asset,\n",
    "        \"mean_rec_strategy_total_returns\": mean_rec_strategy_total_returns,\n",
    "        \"median_rec_strategy_total_returns\": median_rec_strategy_total_returns,\n",
    "        \"mean_rec_strategy_annualized_return\": mean_rec_strategy_annualized_return,\n",
    "        \"median_rec_strategy_annualized_return\": median_rec_strategy_annualized_return,\n",
    "        \"mean_rec_strategy_volatility\": mean_rec_strategy_volatility,\n",
    "        \"median_rec_strategy_volatility\": median_rec_strategy_volatility,\n",
    "        \"mean_rec_strategy_sharpe\": mean_rec_strategy_sharpe,\n",
    "        \"median_rec_strategy_sharpe\": median_rec_strategy_sharpe,\n",
    "        \"mean_rec_strategy_winrate\": mean_rec_strategy_winrate,\n",
    "        \"median_rec_strategy_winrate\": median_rec_strategy_winrate\n",
    "    }\n",
    "    recession_df.append(metrics)\n",
    "\n",
    "recession_df = pd.DataFrame(recession_df)\n",
    "recession_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cd830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_recession_df = []\n",
    "\n",
    "for asset in asset_list:\n",
    "    df = data[asset]\n",
    "    mean_non_rec_strategy_total_returns = df[\"nonrec_strategy total return\"].mean()\n",
    "    median_non_rec_strategy_total_returns = df[\"nonrec_strategy total return\"].median()\n",
    "    mean_non_rec_strategy_annualized_return = df[\"nonrec_ann. strategy return\"].mean()\n",
    "    median_non_rec_strategy_annualized_return = df[\"nonrec_ann. strategy return\"].median()\n",
    "    mean_non_rec_strategy_volatility = df[\"nonrec_ann. strategy volatility\"].mean()\n",
    "    median_non_rec_strategy_volatility = df[\"nonrec_ann. strategy volatility\"].median()\n",
    "    mean_non_rec_strategy_sharpe = df[\"nonrec_strategy sharpe ratio\"].mean()\n",
    "    median_non_rec_strategy_sharpe = df[\"nonrec_strategy sharpe ratio\"].median()\n",
    "    mean_non_rec_strategy_winrate = df[\"nonrec_strategy winrate\"].mean()\n",
    "    median_non_rec_strategy_winrate = df[\"nonrec_strategy winrate\"].median()\n",
    "\n",
    "    metrics = {\n",
    "        \"asset\": asset,\n",
    "        \"mean_non_rec_strategy_total_returns\": mean_non_rec_strategy_total_returns,\n",
    "        \"median_non_rec_strategy_total_returns\": median_non_rec_strategy_total_returns,\n",
    "        \"mean_non_rec_strategy_annualized_return\": mean_non_rec_strategy_annualized_return,\n",
    "        \"median_non_rec_strategy_annualized_return\": median_non_rec_strategy_annualized_return,\n",
    "        \"mean_non_rec_strategy_volatility\": mean_non_rec_strategy_volatility,\n",
    "        \"median_non_rec_strategy_volatility\": median_non_rec_strategy_volatility,\n",
    "        \"mean_non_rec_strategy_sharpe\": mean_non_rec_strategy_sharpe,\n",
    "        \"median_non_rec_strategy_sharpe\": median_non_rec_strategy_sharpe,\n",
    "        \"mean_non_rec_strategy_winrate\": mean_non_rec_strategy_winrate,\n",
    "        \"median_non_rec_strategy_winrate\": median_non_rec_strategy_winrate\n",
    "    }\n",
    "    non_recession_df.append(metrics)\n",
    "\n",
    "non_recession_df = pd.DataFrame(non_recession_df)\n",
    "non_recession_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f308ec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"{strategy_name}\"\n",
    "performance_df.to_excel(f\"performance_summary_{strategy}.xlsx\", index=False)\n",
    "significance_df.to_excel(f\"significance_summary_{strategy}.xlsx\", index=False)\n",
    "significance_counts_df.to_excel(f\"significance_counts_summary_{strategy}.xlsx\", index=False)\n",
    "high_vol_df.to_excel(f\"high_vol_summary_{strategy}.xlsx\", index=False)\n",
    "low_vol_df.to_excel(f\"low_vol_summary_{strategy}.xlsx\", index=False)\n",
    "recession_df.to_excel(f\"recession_summary_{strategy}.xlsx\", index=False)\n",
    "non_recession_df.to_excel(f\"non_recession_summary_{strategy}.xlsx\", index=False)\n",
    "\n",
    "# high_vol_buy_n_hold_df.to_excel(f\"high_vol_buy_n_hold_summary_{strategy}.xlsx\", index=False)\n",
    "# low_vol_buy_n_hold_df.to_excel(f\"low_vol_buy_n_hold_summary_{strategy}.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
